{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_eval_app_data_file = 'allresponses.json'\n",
    "with open(caption_eval_app_data_file) as f:\n",
    "    caption_eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039\n",
      "10631\n",
      "{'Image': 'http://images.cocodataset.org/val2014/COCO_val2014_000000056344.jpg', 'Caption': 'a computer screen is shown with a tv and a computer .', 'CapID': 7773, 'Consensus': '3.0000', 'Z0': 3, 'Z1': 3}\n",
      "{'Image': 'http://images.cocodataset.org/val2014/COCO_val2014_000000026323.jpg', 'Caption': 'A herd of giraffe walking across a grass covered field.', 'CapID': 27790, 'Consensus': '5.0000'}\n"
     ]
    }
   ],
   "source": [
    "count_human = 0\n",
    "for thing in caption_eval_data:\n",
    "    if 'Z2' in thing:\n",
    "        count_human += 1\n",
    "\n",
    "total_rated = len(caption_eval_data)\n",
    "print(total_rated)\n",
    "print(count_human)\n",
    "print(caption_eval_data[1089])\n",
    "print(caption_eval_data[4036])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_ratings = []\n",
    "human_ratings = []\n",
    "\n",
    "no_human = 0\n",
    "for obj in caption_eval_data:\n",
    "    if 'Z0' in obj:\n",
    "        cosine_rating = obj['Z0']\n",
    "    elif 'Z1' in obj:\n",
    "        cosine_rating = obj['Z1']\n",
    "    else:\n",
    "        cosine_rating = obj['Consensus']\n",
    "    cosine_sim_ratings.append(cosine_rating)\n",
    "    total_human_ratings = 0\n",
    "    human_sum = 0\n",
    "    z_val = 2\n",
    "    while True:\n",
    "        key = 'Z'+str(z_val)\n",
    "        if key in obj:\n",
    "            total_human_ratings += 1\n",
    "            human_sum += int(obj[key])\n",
    "            z_val += 1\n",
    "        else:\n",
    "            if z_val == 2:\n",
    "                no_human += 1\n",
    "            break\n",
    "    \n",
    "    if total_human_ratings > 0:\n",
    "        human_average = human_sum / total_human_ratings\n",
    "    else:\n",
    "        human_average = -1\n",
    "    \n",
    "    human_ratings.append(human_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039\n",
      "12039\n",
      "1408\n",
      "1408\n"
     ]
    }
   ],
   "source": [
    "print(len(cosine_sim_ratings))\n",
    "print(len(human_ratings))\n",
    "print(human_ratings.count(-1))\n",
    "print(no_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples without cosine similarity data =  0\n",
      "Number of samples without human data =  1408\n"
     ]
    }
   ],
   "source": [
    "count_of_no_cosine = 0\n",
    "for idx, rat in enumerate(cosine_sim_ratings):\n",
    "    if rat == -1:\n",
    "        print(idx)\n",
    "        count_of_no_cosine += 1\n",
    "        \n",
    "print('Number of samples without cosine similarity data = ', count_of_no_cosine)\n",
    "\n",
    "count_of_no_human = 0\n",
    "for rat in human_ratings:\n",
    "    if rat == -1:\n",
    "        count_of_no_human += 1\n",
    "        \n",
    "print('Number of samples without human data = ', count_of_no_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total with both cosine similarity and human ratings =  10631\n",
      "Total exact match =  5271\n",
      "Accuracy =  0.4958141284921456\n",
      "---------------------------\n",
      "Totals by rating sum =  10631\n",
      "Totals by rating =  [0, 2316, 1895, 1335, 3656, 1429]\n",
      "Correct by rating =  [0, 1761, 632, 482, 1721, 675]\n",
      "Accuracy by rating 1 - 5\n",
      "Rating =  1 | Accuracy =  0.7603626943005182\n",
      "Rating =  2 | Accuracy =  0.33350923482849604\n",
      "Rating =  3 | Accuracy =  0.3610486891385768\n",
      "Rating =  4 | Accuracy =  0.47073304157549234\n",
      "Rating =  5 | Accuracy =  0.4723582925122463\n"
     ]
    }
   ],
   "source": [
    "# Totals_by_rating means the total number of times\n",
    "# Need confusion matrix? Can do if we assume human to be ground truth\n",
    "# This is tricky because we did not use cosine similarity for known 1s (mismatched)\n",
    "# Unclear which \"totals\" to take. The cosine similarity totals? or the human totals? \n",
    "totals_by_rating = [0, 0, 0, 0, 0, 0]\n",
    "correct_by_rating = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "total_with_cos_and_human = 0\n",
    "total_correct = 0\n",
    "for idx, (human, cos) in enumerate(zip(human_ratings, cosine_sim_ratings)):\n",
    "    if human == -1 or cos == -1:\n",
    "        continue\n",
    "    if round(human) == int(cos):\n",
    "        total_correct += 1\n",
    "        correct_by_rating[int(cos)] += 1\n",
    "    \n",
    "    total_with_cos_and_human += 1\n",
    "    #totals_by_rating[int(cos)] += 1\n",
    "    totals_by_rating[round(human)] += 1\n",
    "        \n",
    "accuracy = total_correct / total_with_cos_and_human\n",
    "\n",
    "print('Total with both cosine similarity and human ratings = ', total_with_cos_and_human)\n",
    "print('Total exact match = ', total_correct)\n",
    "print('Accuracy = ', accuracy)\n",
    "print('---------------------------')\n",
    "print('Totals by rating sum = ', sum(totals_by_rating))\n",
    "print('Totals by rating = ', totals_by_rating)\n",
    "print('Correct by rating = ', correct_by_rating)\n",
    "print('Accuracy by rating 1 - 5')\n",
    "for idx, (correct, total) in enumerate(zip(correct_by_rating, totals_by_rating)):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    print('Rating = ', idx, '| Accuracy = ', correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
